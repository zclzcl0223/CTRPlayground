# MODEL_CONFIG
model_id: Transformer
n_embed: 16
n_hidden: 64
n_head: 4
head_dim: 16
n_layer: 2
activation: relu
bias: False

# TRAINING_CONFIG
loss: bce
metrics: [logloss, AUC]
optimizer: adamW
learning_rate: 1.0e-3
batch_size: 8192
embedding_weight_decay: 1.0e-5
net_weight_decay: 0.0
epochs: 10
decay_expand_rate: 1000
max_grad_norm: 1.0
